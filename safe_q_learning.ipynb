{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.envs.toy_text.utils import categorical_sample\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "\n",
    "class CliffWalkingEnv(Env):\n",
    "    \"\"\"\n",
    "    MODIFIED VERSION OF: \n",
    "    https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py\n",
    "    \n",
    "    MODIFICATIONS:\n",
    "    1. Reward returned is negative distance from new state and goal\n",
    "    2. Boolean is returned indicating if agent has fallen\n",
    "    \n",
    "    DESCRIPTION FROM REFERENCE: \n",
    "    This is a simple implementation of the Gridworld Cliff\n",
    "    reinforcement learning task.\n",
    "    Adapted from Example 6.6 (page 106) from [Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n",
    "    With inspiration from:\n",
    "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "    ### Description\n",
    "    The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "    - [3, 0] as the start at bottom-left\n",
    "    - [3, 11] as the goal at bottom-right\n",
    "    - [3, 1..10] as the cliff at bottom-center\n",
    "    If the agent steps on the cliff it returns to the start.\n",
    "    An episode terminates when the agent reaches the goal.\n",
    "    ### Actions\n",
    "    There are 4 discrete deterministic actions:\n",
    "    - 0: move up\n",
    "    - 1: move right\n",
    "    - 2: move down\n",
    "    - 3: move left\n",
    "    ### Observations\n",
    "    There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal (as this results the end of episode). They remain all the positions of the first 3 rows plus the bottom-left cell.\n",
    "    The observation is simply the current position encoded as [flattened index](https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html).\n",
    "    ### Reward\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward.\n",
    "    ### Arguments\n",
    "    ```\n",
    "    gym.make('CliffWalking-v0')\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\", \"ansi\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shape = (4, 12)\n",
    "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
    "\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 4\n",
    "\n",
    "        # Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._cliff[3, 1:-1] = True\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "        # Calculate initial state distribution\n",
    "        # We always start in state (3, 0)\n",
    "        self.initial_state_distrib = np.zeros(self.nS)\n",
    "        self.initial_state_distrib[self.start_state_index] = 1.0\n",
    "\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        \"\"\"\n",
    "        Prevent the agent from falling out of the grid world\n",
    "        :param coord:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        :param current: Current position on the grid as (row, col)\n",
    "        :param delta: Change in position for transition\n",
    "        :return: (1.0, new_state, reward, done)\n",
    "        \"\"\"\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        \n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            reward = -12\n",
    "            return [(1.0, self.start_state_index, reward, False, True)]\n",
    "\n",
    "        is_done = tuple(new_position) == terminal_state\n",
    "        reward =  -1 * math.sqrt((new_position[0] - terminal_state[0])**2 + (new_position[1] - terminal_state[1])**2)\n",
    "        return [(1.0, new_state, reward, is_done, False)] \n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d, f = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        return (int(s), r, d, f, {\"prob\": p})\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        return_info: bool = False,\n",
    "        options: Optional[dict] = None\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
    "        self.lastaction = None\n",
    "        if not return_info:\n",
    "            return int(self.s)\n",
    "        else:\n",
    "            return int(self.s), {\"prob\": 1}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            # Print terminal state\n",
    "            elif position == (3, 11):\n",
    "                output = \" T \"\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += \"\\n\"\n",
    "\n",
    "            outfile.write(output)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "        # No need to return anything for human\n",
    "        if mode != \"human\":\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\", \"episode_falls\", \"target_reward\"])\n",
    "\n",
    "def plot_average_stats(stats, secondStats):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths, label=\"Safe Q-learning\")\n",
    "    plt.plot(secondStats.episode_lengths, label=\"Greedy Q-learning\")\n",
    "    plt.xlabel(\"Simulation\")\n",
    "    plt.ylabel(\"Average episode length\")\n",
    "    plt.title(\"Average episode length per simulation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(len(stats.episode_lengths)), stats.episode_rewards, label=\"Safe Q-learning\")\n",
    "    plt.plot(np.arange(len(secondStats.episode_lengths)), secondStats.episode_rewards, label=\"Greedy Q-learning\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Simulation\")\n",
    "    plt.ylabel(\"Average reward\")\n",
    "    plt.title(\"Average reward per simulation\")\n",
    "    plt.show(fig2)\n",
    "        \n",
    "    # Plot number of falls\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(len(stats.episode_lengths)), stats.episode_falls, label=\"Safe Q-learning\")\n",
    "    plt.plot(np.arange(len(secondStats.episode_lengths)), secondStats.episode_falls, label=\"Greedy Q-learning\")\n",
    "    plt.ylabel(\"Average falls\")\n",
    "    plt.xlabel(\"Simulation\")\n",
    "    plt.title(\"Average falls per simulation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show(fig3)\n",
    "        \n",
    "    # Plot number of falls vs reward\n",
    "    fig4 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_falls, stats.episode_rewards, label=\"Safe Q-learning\")\n",
    "    plt.plot(secondStats.episode_falls, secondStats.episode_rewards, label=\"Greedy Q-learning\")\n",
    "    plt.ylabel(\"Average reward\")\n",
    "    plt.xlabel(\"Average falls\")\n",
    "    plt.title(\"Average falls vs. Average reward\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show(fig4)\n",
    "    \n",
    "    return\n",
    "\n",
    "def plot_episode_stats(stats, secondStats):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths, label=\"Safe Q-learning\")\n",
    "    plt.plot(secondStats.episode_lengths, label=\"Greedy Q-learning\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode length\")\n",
    "    plt.title(\"Episode length over time\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show(fig1)\n",
    "        \n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(len(stats.episode_lengths)), stats.episode_rewards, label=\"Safe Q-learning\")\n",
    "    plt.plot(np.arange(len(secondStats.episode_lengths)), secondStats.episode_rewards, label=\"Greedy Q-learning\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode reward over time\")\n",
    "    plt.show(fig2)\n",
    "        \n",
    "    # Plot number of falls\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(len(stats.episode_lengths)), stats.episode_falls, label=\"Safe Q-learning\")\n",
    "    plt.plot(np.arange(len(secondStats.episode_lengths)), secondStats.episode_falls, label=\"Greedy Q-learning\")\n",
    "    plt.ylabel(\"Falls\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.title(\"Falls per episode over time\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show(fig3)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "  \n",
    "from collections import defaultdict\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Reference: https://www.geeksforgeeks.org/q-learning-in-python/\n",
    "    \"\"\"\n",
    "    def policyFunction(state):\n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions\n",
    "                  \n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        \n",
    "        action = np.random.choice(np.arange(\n",
    "                    len(Action_probabilities)),\n",
    "                    p = Action_probabilities)\n",
    "        \n",
    "        return action\n",
    "   \n",
    "    return policyFunction\n",
    "\n",
    "def safePolicyFunction(Qb, num_actions):\n",
    "    def policyFunction(state, epsilon):\n",
    "        \n",
    "        shouldTakeBestAction = random.random() > epsilon\n",
    "                \n",
    "        if shouldTakeBestAction:\n",
    "            return np.argmax(Qb[state])\n",
    "        else:\n",
    "            random_action = np.random.choice(np.arange(num_actions))\n",
    "            count = 0\n",
    "            while Qb[state][random_action] < -50 and count <= num_actions * 2:\n",
    "                random_action = np.random.choice(np.arange(num_actions))\n",
    "                count = count + 1\n",
    "            return random_action\n",
    "                                \n",
    "    return policyFunction\n",
    "\n",
    "def targetPolicyFunction(Qt, num_actions):\n",
    "    def policyFunction(state):\n",
    "            return np.argmax(Qt[state])\n",
    "    return policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalQLearning(env, num_episodes, policy, discount_factor = 1, alpha = 0.6, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Reference: https://www.geeksforgeeks.org/q-learning-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = defaultdict(lambda: np.full(env.action_space.n, -1))\n",
    "\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths = np.zeros(num_episodes),\n",
    "        episode_rewards = np.zeros(num_episodes),\n",
    "        episode_falls = np.zeros(num_episodes),\n",
    "        target_reward = np.zeros(num_episodes)\n",
    "    )\n",
    "\n",
    "    policy = policy(Q, epsilon, env.action_space.n)\n",
    "\n",
    "    # For every episode\n",
    "    for ith_episode in range(num_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            action = policy(state)\n",
    "            \n",
    "            next_state, reward, done, hasFallen, _ = env.step(action)\n",
    "            \n",
    "            stats.target_reward[ith_episode] += reward\n",
    "            stats.episode_rewards[ith_episode] += reward\n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "            stats.episode_falls[ith_episode] += 1 if hasFallen else 0\n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeQLearning(env, num_episodes, policy, epsilon = 0.1, discount_factor = 1, alpha = 0.6):\n",
    "\n",
    "    # Initialize Qt(s, a) and Qb(s, a)\n",
    "    Qt = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "    Qb = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Initialize episode statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths = np.zeros(num_episodes),\n",
    "        episode_rewards = np.zeros(num_episodes), \n",
    "        episode_falls = np.zeros(num_episodes),\n",
    "        target_reward = np.zeros(num_episodes)\n",
    "    )\n",
    "    \n",
    "    policy = policy(Qb, env.action_space.n)\n",
    "\n",
    "    # Loop for each episode\n",
    "    for ith_episode in range(num_episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        state = env.reset() \n",
    "        \n",
    "        # Initialize epsilon\n",
    "        epsilon = 0.1\n",
    "\n",
    "        # Loop for each step of the episode\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Choose A from S using policy\n",
    "            action = policy(state, epsilon)\n",
    "\n",
    "            # Perform action A and observe R and S' returned from environment\n",
    "            next_state, reward, done, hasFallen, _ = env.step(action)\n",
    "            \n",
    "            # Modify rewards into Rt and Rb\n",
    "            Rt = reward\n",
    "            Rb = -100 if hasFallen else -1\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.target_reward[ith_episode] += Rt\n",
    "            stats.episode_rewards[ith_episode] += reward\n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "            stats.episode_falls[ith_episode] += 1 if hasFallen else 0\n",
    "\n",
    "            # TD Update for Qt\n",
    "            best_next_action = np.argmax(Qt[next_state])\n",
    "            td_target = Rt + discount_factor * Qt[next_state][best_next_action]\n",
    "            td_delta = td_target - Qt[state][action]\n",
    "            Qt[state][action] += alpha * td_delta\n",
    "            \n",
    "            # TD Update for Qb\n",
    "            best_next_action = np.argmax(Qb[next_state])\n",
    "            td_target = Rb + discount_factor * Qb[next_state][best_next_action]\n",
    "            td_delta = td_target - Qb[state][action]\n",
    "            Qb[state][action] += alpha * td_delta\n",
    "\n",
    "            # If agent reached goal then end the simulation\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "            \n",
    "            # Update epsilon\n",
    "            epsilon = epsilon * (1 - 1 / num_episodes)\n",
    "\n",
    "    return Qt, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQLearning(env, num_episodes, policy, Qt, discount_factor = 1, alpha = 0.6):\n",
    "    print(\"TESTING\")\n",
    "\n",
    "    # Initialize episode statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths = np.zeros(num_episodes),\n",
    "        episode_rewards = np.zeros(num_episodes), \n",
    "        episode_falls = np.zeros(num_episodes),\n",
    "        target_reward = np.zeros(num_episodes)\n",
    "    )\n",
    "    \n",
    "    policy = policy(Qt, env.action_space.n)\n",
    "\n",
    "    # Loop for each episode\n",
    "    for ith_episode in range(num_episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        state = env.reset() \n",
    "\n",
    "        # Loop for each step of the episode\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Choose A from S using policy\n",
    "            action = policy(state)\n",
    "\n",
    "            # Perform action A and observe R and S' returned from environment\n",
    "            next_state, reward, done, hasFallen, _ = env.step(action)\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.target_reward[ith_episode] += reward\n",
    "            stats.episode_rewards[ith_episode] += reward\n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "            stats.episode_falls[ith_episode] += 1 if hasFallen else 0\n",
    "            \n",
    "            best_next_action = np.argmax(Qt[next_state])\n",
    "            td_target = reward + discount_factor * Qt[next_state][best_next_action]\n",
    "            td_delta = td_target - Qt[state][action]\n",
    "            Qt[state][action] += alpha * td_delta\n",
    "\n",
    "            # If agent reached goal then end the simulation\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "    return Qt, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodeStatsWithArrays():\n",
    "    return EpisodeStats(\n",
    "        episode_lengths = [],\n",
    "        episode_rewards = [], \n",
    "        episode_falls = [],\n",
    "        target_reward = []\n",
    "    )\n",
    "\n",
    "def getEpisodeStatsWithZeroes(number):\n",
    "    return EpisodeStats(\n",
    "        episode_lengths = np.zeros(number),\n",
    "        episode_rewards = np.zeros(number), \n",
    "        episode_falls = np.zeros(number),\n",
    "        target_reward = np.zeros(number)\n",
    "    )\n",
    "\n",
    "def appendValuesToArray(array, values):\n",
    "    array.episode_lengths.append(values.episode_lengths)\n",
    "    array.episode_rewards.append(values.episode_rewards)\n",
    "    array.episode_falls.append(values.episode_falls)\n",
    "    array.target_reward.append(values.target_reward)\n",
    "    \n",
    "def appendLastValuesToArray(array, values):\n",
    "    array.episode_lengths.append(values.episode_lengths[49:])\n",
    "    array.episode_rewards.append(values.episode_rewards[49:])\n",
    "    array.episode_falls.append(values.episode_falls[49:])\n",
    "    array.target_reward.append(values.target_reward[49:])\n",
    "    \n",
    "def calculateAverageOfArray(array, values):\n",
    "    for i in range(len(values.episode_lengths)):\n",
    "        array.episode_lengths[i] = np.average(values.episode_lengths[i])\n",
    "        array.episode_rewards[i] = np.average(values.episode_rewards[i])\n",
    "        array.episode_falls[i] = np.average(values.episode_falls[i])\n",
    "        array.target_reward[i] = np.average(values.target_reward[i])\n",
    "        \n",
    "def appendValuesToEpisode(array, values):\n",
    "    return EpisodeStats(np.add(array.episode_lengths, values.episode_lengths),\n",
    "                        np.add(array.episode_rewards, values.episode_rewards),\n",
    "                        np.add(array.episode_falls, values.episode_falls),\n",
    "                        np.add(array.target_reward, values.target_reward)\n",
    "\n",
    "            )\n",
    "  \n",
    "def calculateAverageOfTimestep(array, num_episodes, num_sim):\n",
    "    for i in range(num_episodes):\n",
    "        array.episode_lengths[i] = array.episode_lengths[i] / num_sim\n",
    "        array.episode_rewards[i] = array.episode_rewards[i] / num_sim\n",
    "        array.episode_falls[i] = array.episode_falls[i] / num_sim\n",
    "        array.target_reward[i] = array.target_reward[i] / num_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStatistics(average_safe_statistics, average_greedy_statistics,): \n",
    "    print(\"Episode length\")\n",
    "    print(\"Average: [GREEDY] \" + str(np.average(average_greedy_statistics.episode_lengths)) + \" [SAFE] \" + str(np.average(average_safe_statistics.episode_lengths)))\n",
    "    print(\"Variance: [GREEDY] \" + str(np.var(average_greedy_statistics.episode_lengths)) + \" [SAFE] \" + str(np.var(average_safe_statistics.episode_lengths)))\n",
    "    \n",
    "    print(\"\\nTotal reward\")\n",
    "    print(\"Average: [GREEDY] \" + str(np.average(average_greedy_statistics.episode_rewards)) + \" [SAFE] \" + str(np.average(average_safe_statistics.episode_rewards)))\n",
    "    print(\"Variance: [GREEDY] \" + str(np.var(average_greedy_statistics.episode_rewards)) + \" [SAFE] \" + str(np.var(average_safe_statistics.episode_rewards)))\n",
    "    \n",
    "    print(\"\\nNumber of falls\")\n",
    "    print(\"Average: [GREEDY] \" + str(np.average(average_greedy_statistics.episode_falls)) + \" [SAFE] \" + str(np.average(average_safe_statistics.episode_falls)))\n",
    "    print(\"Variance: [GREEDY] \" + str(np.var(average_greedy_statistics.episode_falls)) + \" [SAFE] \" + str(np.var(average_safe_statistics.episode_falls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainModel(): \n",
    "    num_sim = 100\n",
    "    num_episodes = 100\n",
    "    safeQTables = []\n",
    "    greedyQTables = []\n",
    "\n",
    "    safe_statistics = getEpisodeStatsWithArrays()\n",
    "    greedy_statistics = getEpisodeStatsWithArrays()\n",
    "\n",
    "    average_safe_statistics = getEpisodeStatsWithZeroes(num_sim)\n",
    "    average_greedy_statistics = getEpisodeStatsWithZeroes(num_sim)\n",
    "\n",
    "    time_step_safe = getEpisodeStatsWithZeroes(num_episodes)\n",
    "    time_step_greedy = getEpisodeStatsWithZeroes(num_episodes)\n",
    "\n",
    "    for i in range(num_sim):\n",
    "        Qt, stats = safeQLearning(env, num_episodes, safePolicyFunction)\n",
    "        Qe, eStats = originalQLearning(env, num_episodes, createEpsilonGreedyPolicy)\n",
    "        safeQTables.append(Qt)\n",
    "        greedyQTables.append(Qe)\n",
    "\n",
    "        appendLastValuesToArray(safe_statistics, stats)\n",
    "        appendLastValuesToArray(greedy_statistics, eStats)\n",
    "\n",
    "        time_step_safe = appendValuesToEpisode(time_step_safe, stats)\n",
    "        time_step_greedy = appendValuesToEpisode(time_step_greedy, eStats)\n",
    "\n",
    "    calculateAverageOfArray(average_safe_statistics, safe_statistics)\n",
    "    calculateAverageOfArray(average_greedy_statistics, greedy_statistics)\n",
    "\n",
    "    calculateAverageOfTimestep(time_step_safe, num_episodes, num_sim)\n",
    "    calculateAverageOfTimestep(time_step_greedy, num_episodes, num_sim)\n",
    "\n",
    "    plot_episode_stats(time_step_safe, time_step_greedy)\n",
    "    plot_average_stats(average_safe_statistics, average_greedy_statistics)\n",
    "    \n",
    "    printStatistics(average_safe_statistics, average_greedy_statistics)\n",
    "    \n",
    "    return safeQTables, greedyQTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(safeQTables, greedyQTables):\n",
    "    num_episodes = 1\n",
    "    num_sim = 100\n",
    "    \n",
    "    safe_statistics = getEpisodeStatsWithArrays()\n",
    "    greedy_statistics = getEpisodeStatsWithArrays()\n",
    "    \n",
    "    average_safe_statistics = getEpisodeStatsWithZeroes(num_sim)\n",
    "    average_greedy_statistics = getEpisodeStatsWithZeroes(num_sim)\n",
    "    \n",
    "    for i in range(num_sim):\n",
    "        Qt, stats = testQLearning(env, num_episodes, targetPolicyFunction, safeQTables[i])\n",
    "        Qe, eStats = testQLearning(env, num_episodes, targetPolicyFunction, greedyQTables[i])\n",
    "        \n",
    "        appendValuesToArray(safe_statistics, stats)\n",
    "        appendValuesToArray(greedy_statistics, eStats)\n",
    "    \n",
    "    calculateAverageOfArray(average_safe_statistics, safe_statistics)\n",
    "    calculateAverageOfArray(average_greedy_statistics, greedy_statistics)\n",
    "\n",
    "    plot_average_stats(average_safe_statistics, average_greedy_statistics)\n",
    "    printStatistics(average_safe_statistics, average_greedy_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeQTables, greedyQTables = trainModel()\n",
    "testModel(safeQTables, greedyQTables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
